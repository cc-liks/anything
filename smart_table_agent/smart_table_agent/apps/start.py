from smart_table_agent.file_processing.file_manager import FileManager
from smart_table_agent.models_manager.model_manager import ModelManager


class SmartTableAgent:

    def __init__(self):
        self.model_manager = ModelManager()
        self.file_manager = FileManager()
        self._init_info()

    def _init_info(self):
        self.model_manager.register_model("test_model", "DeepSeek")

    def run(self):
        pass


# # -----------------------------
# # 1. è¯»å–æœ¬åœ°æ–‡ä»¶
# # -----------------------------
# def read_file(path):
#     with open(path, "r", encoding="utf-8") as f:
#         return f.read()
#
#
# # -----------------------------
# # 2. æ–‡æœ¬åˆ‡åˆ†
# # -----------------------------
# def split_text(text, chunk_size=400):
#     chunks = []
#     start = 0
#     while start < len(text):
#         end = min(start + chunk_size, len(text))
#         chunks.append(text[start:end])
#         start = end
#     return chunks
#
#
# # -----------------------------
# # 3. æ„å»º Chroma å‘é‡åº“
# # -----------------------------
# def build_chroma(chunks):
#     chroma_client = chromadb.Client(Settings(chroma_db_impl="duckdb+parquet",
#                                              persist_directory="./rag_db"))
#
#     collection = chroma_client.get_or_create_collection(
#         name="rag_collection",
#         metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
#     )
#
#     ids = [f"doc_{i}" for i in range(len(chunks))]
#
#     print("æ­£åœ¨ç”Ÿæˆ Embeddings ...")
#
#     embeddings = []
#     batch_size = 32
#
#     for i in range(0, len(chunks), batch_size):
#         batch = chunks[i:i + batch_size]
#
#         response = client.embeddings.create(
#             model="text-embedding-3-small",
#             input=batch
#         )
#         emb = [e.embedding for e in response.data]
#         embeddings.extend(emb)
#
#     collection.add(
#         documents=chunks,
#         ids=ids,
#         embeddings=embeddings
#     )
#
#     print("å‘é‡åº“æ„å»ºå®Œæˆã€‚")
#     return collection
#
#
# # -----------------------------
# # 4. æ£€ç´¢ + ç”Ÿæˆå›ç­”
# # -----------------------------
# def query_rag(collection, query):
#     # ç”Ÿæˆ Query Embedding
#     query_emb = client.embeddings.create(
#         model="text-embedding-3-small",
#         input=query
#     ).data[0].embedding
#
#     # æ£€ç´¢ç›¸å…³ç‰‡æ®µ
#     results = collection.query(
#         query_embeddings=[query_emb],
#         n_results=3
#     )
#
#     context = "\n\n".join(results["documents"][0])
#
#     # å°†ä¸Šä¸‹æ–‡ + é—®é¢˜å‘ç»™å¤§æ¨¡å‹
#     response = client.chat.completions.create(
#         model="gpt-4o-mini",
#         messages=[
#             {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šæ–‡æ¡£åŠ©æ‰‹ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚"},
#             {"role": "user", "content": f"æ–‡æ¡£å†…å®¹ï¼š\n{context}\n\né—®é¢˜ï¼š{query}"}
#         ]
#     )
#
#     return response.choices[0].message.content
#
#
# # -----------------------------
# # 5. ä¸»è¿è¡Œæµç¨‹
# # -----------------------------
# if __name__ == "__main__":
#     file_path = "example.txt"   # ä¿®æ”¹ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„
#     text = read_file(file_path)
#
#     chunks = split_text(text)
#
#     collection = build_chroma(chunks)
#
#     print("\n========= RAG é—®ç­”ç³»ç»Ÿ =========")
#     while True:
#         query = input("\nè¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰ï¼š")
#         if query.lower() == "exit":
#             break
#         answer = query_rag(collection, query)
#         print("\nğŸ’¡ å›ç­”ï¼š\n", answer)


def start_main():
    smart_table_agent = SmartTableAgent()
    smart_table_agent.run()
